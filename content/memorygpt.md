+++
title = 'MemoryGPT - Agentic Approach to building a GPT-esque application'
date = 2024-12-10T22:48:58+05:30
draft = true
description = "The post is about efficiently utilizing context length in LLMs and addressing limitations on the same and I aim to solve issues arise with information impedance and catastrophic information loss. This post will track all the work I perform towards it."
image = "/images/langchain_context/concept_arch.png"
imageBig = "/images/langchain_context.jpg"
categories = ["langchain","llm","langgraph","ai"]
authors = ["Raghul"]
avatar = "/images/avatar.webp"
+++

## A Rough Idea - 
Here is the concept architecture diagram for the proposed solution.
![Langchain Concept Architecture](/images/langchain_context/concept_arch.png)

## Links - 
[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172)
